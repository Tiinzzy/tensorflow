{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7d32e2-5571-4253-b6e6-4998f4803d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 17:44:24.026616: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-13 17:44:24.061849: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-13 17:44:24.062689: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-13 17:44:24.678510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import models, layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ae85f27-3e44-4332-b3e0-7e3ddaaffffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 5\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"5\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"5\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"5\"\n",
    "tf.config.threading.set_inter_op_parallelism_threads(num_threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(num_threads)\n",
    "tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db4fe10c-41d5-4627-8a3c-c7efb229f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./data/recipe-dataset.csv')\n",
    "texts = dataset['text'].astype(str)\n",
    "labels = dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d723d943-5291-4bf8-90ce-9285c2d29342",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=max_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fb83fe1-2646-4328-9636-b1e2e76ee89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,  70, 131,  22],\n",
       "       [  0,   0,   0, ...,  15,  64,  22],\n",
       "       [  0,   0,   0, ...,  12,  17,  69],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  44,  50,  25],\n",
       "       [  0,   0,   0, ...,   7,  10, 230],\n",
       "       [  0,   0,   0, ...,  13, 394,   2]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c487551-1753-4427-9586-02bae69cef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array(labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b1133c9-d298-410d-b28f-e8ab2c2adb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = data[:1500] \n",
    "test_y = targets[:1500]\n",
    "train_x = data[1500:]\n",
    "train_y = targets[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba7c42bf-54a6-4968-807b-ead8eb8e2331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1828, 500), (1828,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80c10806-384c-4540-9e7b-94ee8d7e07d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(959, 134, 500)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts[0]), len(sequences[0]), len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa863eb9-2b26-44f6-a08a-b1e4306efee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                25050     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30201 (117.97 KB)\n",
      "Trainable params: 30201 (117.97 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 18:04:08.607242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-13 18:04:08.608321: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation=\"relu\", input_shape=(max_words,)))\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43cc1678-bce8-477b-b1e9-3fc862ad0ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "58/58 [==============================] - 1s 4ms/step - loss: 1.4569 - accuracy: 0.7609 - val_loss: 0.3527 - val_accuracy: 0.9000\n",
      "Epoch 2/5\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.6537 - accuracy: 0.8518 - val_loss: 0.2527 - val_accuracy: 0.9120\n",
      "Epoch 3/5\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.8638 - val_loss: 0.2369 - val_accuracy: 0.9113\n",
      "Epoch 4/5\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8846 - val_loss: 0.2247 - val_accuracy: 0.9207\n",
      "Epoch 5/5\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.8961 - val_loss: 0.2275 - val_accuracy: 0.9140\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "results = model.fit(train_x, train_y, epochs=5, batch_size=32, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a662119-f4ca-4a12-aa0b-b202ecb03ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.40%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e4dcc8a-9bbe-4125-81bf-15eadbfe5fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/keras-recipe-sentiment.mdl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/keras-recipe-sentiment.mdl/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model Accuracy: 91.40%\n"
     ]
    }
   ],
   "source": [
    "model.save('./data/keras-recipe-sentiment.mdl')\n",
    "loaded_model = models.load_model('./data/keras-recipe-sentiment.mdl')\n",
    "scores = loaded_model.evaluate(test_x, test_y, verbose=0)\n",
    "print(\"Loaded Model Accuracy: %.2f%%\" % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "83cec5c9-4046-4319-a190-9c9b1120a119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "[[1.3444057e-05]]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "4 medium sweet potatoes\n",
    "1 tablespoon unsalted butter, melted\n",
    "1 teaspoon olive oil\n",
    "1 teaspoon finely chopped fresh thyme leaves\n",
    "1 garlic clove, finely grated on a microplane\n",
    "Kosher salt and freshly ground black pepper\n",
    "1/3 cup nonfat Greek-style yogurt\n",
    "1 scallion, white and green parts chopped\n",
    "\"\"\"\n",
    "\n",
    "input_sequence = tokenizer.texts_to_sequences([input_text])\n",
    "input_data = pad_sequences(input_sequence, maxlen=max_words)\n",
    "prediction = loaded_model.predict(input_data)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eaa4bf-1dfd-4277-9eab-206c3185ee57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40df0d07-d550-4d00-9090-1ff17f3e0ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387.8722956730769"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(t) for t in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59760169-4095-427a-b813-67e714d19456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load vectors directly from the file\n",
    "# model = KeyedVectors.load_word2vec_format('path/to/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model = api.load('word2vec-google-news-300')\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nb_words = 500\n",
    "embed_size = 500\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_words, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    if word in model.vocab:\n",
    "        embedding_matrix[i] = model.word_vec(word)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(nb_words,\n",
    "                            embed_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe75804-3a68-48a0-8dc9-c52ca35539ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(layers.Dense(50, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc628b-84a2-4af6-b813-d0d5e14694de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22aa322-3736-499d-a48c-79a012808124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
