{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb60c21-58e7-49b1-950b-58d065830e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "SEQUENCE_LENGTH = 2\n",
    "LENGTH = SEQUENCE_LENGTH + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb8fdd3-61b8-45b8-9b43-d3a0536b9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 5\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"5\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"5\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"5\"\n",
    "tf.config.threading.set_inter_op_parallelism_threads(num_threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(num_threads)\n",
    "tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca3aba5-8b8b-4f0e-8f81-e1be3211f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/tina/Downloads/ai.yml', 'r') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "    \n",
    "# print(data['conversations'])  \n",
    "\n",
    "all_texts = []\n",
    "for inner_list in data['conversations']:\n",
    "    for text in inner_list:\n",
    "        all_texts.append(text)\n",
    "        \n",
    "# print(all_texts)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49346907-0927-451f-bb89-335584d60a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = []\n",
    "\n",
    "pattern = r'[^a-zA-Z\\s]'\n",
    "for text in all_texts:\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    cleaned_texts.append(cleaned_text)\n",
    "# print(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8aeeacd-dc17-4d98-bcf2-d727748c9cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    tokens = [' ' if w in string.punctuation else w for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dfd9ec7-2091-4ee3-8748-858f16735a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1185\n"
     ]
    }
   ],
   "source": [
    "all_tokens = []\n",
    "\n",
    "for text in all_texts:\n",
    "    tokens = clean_doc(text)\n",
    "    all_tokens.extend(tokens)\n",
    "print(len(all_tokens))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01925f21-e68d-4e95-9ef1-6279f6e1c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plato = open(\"/home/tina/Downloads/plato.txt\", \"r\").read()\n",
    "# text = plato\n",
    "# tokens = clean_doc(text)\n",
    "\n",
    "# print (len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf7d476-388f-42f9-acf1-890d1370afb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 5\n",
      "Unique Tokens: 5\n",
      "These are the first 200 tokens: ['i', 'am', 'just', 'an', 'artificial']\n"
     ]
    }
   ],
   "source": [
    "number_of_unique_tokens = len(set(tokens))\n",
    "\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % number_of_unique_tokens)\n",
    "print('These are the first 200 tokens: %s' % tokens[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ed8866-3624-4490-8282-71e6242f2fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 2\n",
      "This is the first sequence: i am just\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 2\n",
    "LENGTH = SEQUENCE_LENGTH + 1\n",
    "sequences = list()\n",
    "for i in range(LENGTH, len(tokens)):\n",
    "    seq = tokens[i-LENGTH:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "\n",
    "print ('Total Sequences: %d' % len(sequences))\n",
    "print ('This is the first sequence: {0}'.format(sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0b7f43e-9614-4cb3-933e-2ed169bda8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "sequences = tokenizer.texts_to_sequences(sequences)\n",
    "vocab_size = number_of_unique_tokens + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3faf0646-69cf-4c5a-9eff-007ebc45aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences0 = np.array(sequences)\n",
    "X, y = sequences0[:,:-1], sequences0[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e30f888-9a55-4bc0-8e5c-1cacfe1f7cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 10:25:49.022474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-18 10:25:49.023842: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2, 2)              12        \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 2, 100)            41200     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 606       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 132318 (516.87 KB)\n",
      "Trainable params: 132318 (516.87 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dimensions_to_represent_word = 100\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, SEQUENCE_LENGTH, input_length=SEQUENCE_LENGTH))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd81032e-fb59-496b-90ea-a428254fae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.7917 - accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7873 - accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7816 - accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7758 - accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7696 - accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7630 - accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7558 - accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7481 - accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7396 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7304 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7204 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7095 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.6974 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6842 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6696 - accuracy: 0.5000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6535 - accuracy: 0.5000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6358 - accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6163 - accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.5947 - accuracy: 0.5000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.5711 - accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5450 - accuracy: 0.5000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.5163 - accuracy: 0.5000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4848 - accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4505 - accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4132 - accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3725 - accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3289 - accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2822 - accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2327 - accuracy: 0.5000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1812 - accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1276 - accuracy: 0.5000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0734 - accuracy: 0.5000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0194 - accuracy: 0.5000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9670 - accuracy: 0.5000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9174 - accuracy: 0.5000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8721 - accuracy: 0.5000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8320 - accuracy: 0.5000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7978 - accuracy: 0.5000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7697 - accuracy: 0.5000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7473 - accuracy: 0.5000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7300 - accuracy: 0.5000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7171 - accuracy: 0.5000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7078 - accuracy: 0.5000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7017 - accuracy: 0.5000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6979 - accuracy: 0.5000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6957 - accuracy: 0.5000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6940 - accuracy: 0.5000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6923 - accuracy: 0.5000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6905 - accuracy: 0.5000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6891 - accuracy: 0.5000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6885 - accuracy: 0.5000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6885 - accuracy: 0.5000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6883 - accuracy: 0.5000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6875 - accuracy: 0.5000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6864 - accuracy: 0.5000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6857 - accuracy: 0.5000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6856 - accuracy: 0.5000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6853 - accuracy: 0.5000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6845 - accuracy: 0.5000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6833 - accuracy: 0.5000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6825 - accuracy: 0.5000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6820 - accuracy: 0.5000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6813 - accuracy: 0.5000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6801 - accuracy: 0.5000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6788 - accuracy: 0.5000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6778 - accuracy: 0.5000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6768 - accuracy: 0.5000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6754 - accuracy: 0.5000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6737 - accuracy: 0.5000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6721 - accuracy: 0.5000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6705 - accuracy: 0.5000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6685 - accuracy: 0.5000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6663 - accuracy: 0.5000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6639 - accuracy: 0.5000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6615 - accuracy: 0.5000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6587 - accuracy: 0.5000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6555 - accuracy: 0.5000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6520 - accuracy: 0.5000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6483 - accuracy: 0.5000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6442 - accuracy: 0.5000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6395 - accuracy: 0.5000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6344 - accuracy: 0.5000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6288 - accuracy: 0.5000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6226 - accuracy: 0.5000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6156 - accuracy: 0.5000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6079 - accuracy: 0.5000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5994 - accuracy: 0.5000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5899 - accuracy: 0.5000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5793 - accuracy: 0.5000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5661 - accuracy: 0.5000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5521 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5376 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5210 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5032 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4840 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4630 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4403 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4159 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3896 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3618 - accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./data/chat-model-prediction.mdl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/chat-model-prediction.mdl/assets\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "model.save(\"./data/chat-model-prediction.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbf1fd4a-50b5-41a5-b81c-f49c131ce7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "1/1 [==============================] - 0s 485ms/step\n",
      "(1, 6)\n",
      "[[3.1910190e-06 3.8800863e-05 6.5214884e-01 1.4950907e-05 3.4775102e-01\n",
      "  4.3320964e-05]]\n"
     ]
    }
   ],
   "source": [
    "print (X.shape)\n",
    "prediction = model.predict(X[0].reshape(1,SEQUENCE_LENGTH))\n",
    "print (prediction.shape)\n",
    "print (prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e07e7b3-d233-455c-9d9d-157cb5c04a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'is'], ['are', 'you'], ['im', 'not', 'that']]\n",
      "[[], [], []]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model(\"./data/chat-model-prediction.mdl\")\n",
    "test_texts = [\n",
    "    'what is',\n",
    "    'are you ',\n",
    "    'im not that '\n",
    "]\n",
    "\n",
    "test_sequences = [clean_doc(text) for text in test_texts]\n",
    "print(test_sequences)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sequences)\n",
    "print(test_sequences)\n",
    "\n",
    "for i, test_sequence in enumerate(test_sequences):\n",
    "    if len(test_sequence) < SEQUENCE_LENGTH:\n",
    "        continue\n",
    "    input_sequence = test_sequence[-SEQUENCE_LENGTH:]\n",
    "    input_sequence = np.array(input_sequence).reshape(1, -1)  \n",
    "    predicted_probs = loaded_model.predict(input_sequence)\n",
    "    \n",
    "    predicted_word_index = np.argmax(predicted_probs)\n",
    "    \n",
    "    predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "    print(predicted_word)\n",
    "    print(f\"Input: {' '.join(test_sequence)} | Predicted: {predicted_word}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
